{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r /content/drive/MyDrive/Colab/data /content/data\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab\n",
        "%ls"
      ],
      "metadata": {
        "id": "3dlxtjAuDfKX",
        "outputId": "50d9ab8d-2757-424a-edc5-bf7b65f15608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab\n",
            "\u001b[0m\u001b[01;34mAdGAN\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34msaved_models\u001b[0m/  \u001b[01;34msrc\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "xtoXvC1nfhOK",
        "ExecuteTime": {
          "end_time": "2023-06-04T13:03:44.407877828Z",
          "start_time": "2023-06-04T13:03:42.796806718Z"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.utils as vutils\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "kwb65Q9DfhOO",
        "ExecuteTime": {
          "end_time": "2023-06-04T13:03:45.180199136Z",
          "start_time": "2023-06-04T13:03:45.157605225Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, size_z, num_feature_maps, num_color_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.size_z = size_z\n",
        "        self.network = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.size_z, num_feature_maps * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps * 4, num_feature_maps * 2, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps * 2, num_feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps, num_color_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.network(x)\n",
        "        return output\n",
        "\n",
        "    def gen_shifted(self, x, shift):\n",
        "        shift = torch.unsqueeze(shift, -1)\n",
        "        shift = torch.unsqueeze(shift, -1)\n",
        "        return self.forward(x + shift)\n"
      ],
      "metadata": {
        "id": "wNc602dbgXO3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_feature_maps, num_color_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(num_color_channels, num_feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps, num_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps * 2, num_feature_maps * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        feature = out\n",
        "        out = self.fc(out)\n",
        "        return out.view(-1, 1).squeeze(1), feature"
      ],
      "metadata": {
        "id": "5bkWt3mZgbvB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnoMNIST(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        root_dir = os.path.join(root_dir, \"AnoMNIST\")\n",
        "        assert os.path.exists(os.path.join(root_dir, \"anomnist_dataset.csv\")), \"Invalid root directory\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label = pd.read_csv(os.path.join(root_dir, \"anomnist_dataset.csv\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.label.iloc[idx, 0])\n",
        "        image_label = {\"label\": self.label.iloc[idx, 1], \"anomaly\": self.label.iloc[idx, 2]}\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, image_label\n",
        "\n",
        "\n",
        "class AnomalyExtendedMNIST(datasets.MNIST):\n",
        "    def __getitem__(self, idx):\n",
        "        return super(AnomalyExtendedMNIST, self).__getitem__(idx)[0], {\"label\": super(AnomalyExtendedMNIST, self).__getitem__(idx)[1], \"anomaly\": False}"
      ],
      "metadata": {
        "id": "gdZT3IEJg2V3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ano_mnist_dataset(transform, root_dir, labels=[9], train_size=0.9):\n",
        "    ano_mnist_dataset = AnoMNIST(\n",
        "        root_dir=root_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    mnist_dataset = AnomalyExtendedMNIST(\n",
        "        root=root_dir,\n",
        "        train=True,\n",
        "        transform=transform,\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    dat = torch.utils.data.ConcatDataset([ano_mnist_dataset, mnist_dataset])\n",
        "\n",
        "    if len(labels) > 0:\n",
        "        dat = [d for d in dat if (d[1]['label'] in labels)]\n",
        "\n",
        "    absolute_train_size = int(len(dat) * train_size)\n",
        "    absolute_test_size = len(dat) - absolute_train_size\n",
        "    return torch.utils.data.random_split(dat, [absolute_train_size, absolute_test_size])"
      ],
      "metadata": {
        "id": "a5t0c6FqgiK4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_classes = 1\n",
        "learning_rate = 0.0002\n",
        "num_epochs = 300\n",
        "num_color_channels = 1\n",
        "num_feature_maps_g = 64\n",
        "num_feature_maps_d = 64\n",
        "size_z = 100\n",
        "adam_beta1 = 0.4\n",
        "test_size = 1"
      ],
      "metadata": {
        "id": "1Y3il4ovfhOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "generator = Generator(size_z=size_z,\n",
        "                      num_feature_maps=num_feature_maps_g,\n",
        "                      num_color_channels=num_color_channels).to(device)\n",
        "discriminator = Discriminator(num_feature_maps=num_feature_maps_d,\n",
        "                              num_color_channels=num_color_channels).to(device)"
      ],
      "metadata": {
        "id": "tCj3teR7fhOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(.5,), std=(.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "indices = (train_dataset.targets == 9)\n",
        "train_dataset.data, train_dataset.targets = train_dataset.data[indices], train_dataset.targets[indices]\n",
        "\n",
        "indices = (test_dataset.targets == 9)\n",
        "test_dataset.data, test_dataset.targets = test_dataset.data[indices], test_dataset.targets[indices]\n",
        "\n",
        "ano_mnist_dataset, _ = get_ano_mnist_dataset(transform=transform, root_dir=\"/content/data\", labels=[9])"
      ],
      "metadata": {
        "id": "VlILqAhFfhOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save/Load models into generator and discriminator models"
      ],
      "metadata": {
        "collapsed": false,
        "id": "nNUq7iL5VO0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab/\n",
        "!ls\n",
        "\n",
        "def save_checkpoint(epoch):\n",
        "    timestamp = time.time()\n",
        "    torch.save(generator.state_dict(),f'./saved_models/generator_epoch_{epoch}_{timestamp}.pkl')\n",
        "    torch.save(discriminator.state_dict(),f'./saved_models/discriminator_epoch_{epoch}_{timestamp}.pkl')\n",
        "\n",
        "def save_models():\n",
        "    torch.save(generator.state_dict(),f'./saved_models/generator_latest.pkl')\n",
        "    torch.save(discriminator.state_dict(),f'./saved_models/discriminator_latest.pkl')\n",
        "\n",
        "def load_models():\n",
        "    generator.load_state_dict(torch.load(\"./saved_models/generator.pkl\", map_location=torch.device(device)))\n",
        "    discriminator.load_state_dict(torch.load('./saved_models/discriminator.pkl', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "AUkBkw0LdQCi",
        "outputId": "42753e68-48a6-49b7-bec5-a452f763e9a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab\n",
            "AdGAN  data  saved_models  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the GAN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Mr8Pn8i9VO0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(64, size_z, 1, 1, device=device)\n",
        "\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=learning_rate, betas=(adam_beta1, 0.999))\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(adam_beta1, 0.999))\n",
        "\n",
        "def train_gan(dataset):\n",
        "    img_list = []\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "    iters = 0\n",
        "    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "\n",
        "    print(\"Starting Training Loop...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (real_images, _) in enumerate(dataloader):\n",
        "            # get batch-size from actual image batch\n",
        "            bs = real_images.shape[0]\n",
        "\n",
        "            # -- train discriminator --\n",
        "\n",
        "            # reset/clear discriminators gradient\n",
        "            discriminator.zero_grad()\n",
        "\n",
        "            # move images to either CPU or GPU\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # creates a label tensor filled with 1s\n",
        "            label = torch.full((bs,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "            # get probs for discriminators guess on the real images\n",
        "            output, _ = discriminator(real_images)\n",
        "\n",
        "            # get loss for real images. that means it calculates the difference\n",
        "            # between the output of the model with the current parameter and the\n",
        "            # target (goal) of what the model is supposed to do\n",
        "            # output --> current outcome of the model\n",
        "            # label  --> target of the model\n",
        "            lossD_real = criterion(output, label)\n",
        "\n",
        "            # calculates the gradient (using chain-rule)\n",
        "            # see https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
        "            lossD_real.backward()\n",
        "\n",
        "            # Gets the mean value of all results from the discriminator to get an average\n",
        "            # probability of all sample evaluations (for real data ) --> D(x)\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # create noise as an input for the G in order to create fake images\n",
        "            noise = torch.randn(bs, size_z, 1, 1, device=device)\n",
        "\n",
        "            # use generator to map input noise to an output that is supposed do become fake images during training\n",
        "            fake_images = generator(noise)\n",
        "\n",
        "            # creates a label tensor filled with 0s\n",
        "            label.fill_(fake_label)\n",
        "\n",
        "            # get discriminators guess on fake images\n",
        "            output, _ = discriminator(fake_images.detach())\n",
        "\n",
        "            # get loss for fake images\n",
        "            lossD_fake = criterion(output, label)\n",
        "\n",
        "            # adjust parameter to identify fakes\n",
        "            lossD_fake.backward()\n",
        "\n",
        "            # gets the mean value of all results from the discriminator to get an average\n",
        "            # probability of all sample evaluations. this time for the fake images that were\n",
        "            # generated by the generator --> D(G(z))\n",
        "            D_G_z1 = output.mean().item()\n",
        "\n",
        "            # calculate loss\n",
        "            lossD = lossD_real + lossD_fake\n",
        "\n",
        "            # adjust models (discriminator) parameter\n",
        "            optimizerD.step()\n",
        "\n",
        "            # -- train generator --\n",
        "\n",
        "            # reset/clear generators gradient\n",
        "            generator.zero_grad()\n",
        "\n",
        "            # creates a label tensor filled with 1s\n",
        "            label.fill_(real_label)\n",
        "\n",
        "            # get discriminators guess on fake images\n",
        "            output, _ = discriminator(fake_images)\n",
        "            output = output.view(-1)\n",
        "\n",
        "            # get loss for fake images\n",
        "            lossG = criterion(output, label)\n",
        "\n",
        "            # adjust parameter to generate fakes\n",
        "            lossG.backward()\n",
        "\n",
        "            # gets the mean value of all results from the discriminator to get an average\n",
        "            # probability of all sample evaluations. this time for the fake images that were\n",
        "            # generated by the generator --> D(G(z))\n",
        "            D_G_z2 = output.mean().item()\n",
        "\n",
        "            # adjust models (generator) parameter\n",
        "            optimizerG.step()\n",
        "            # Save Losses for plotting later\n",
        "            G_losses.append(lossG.item())\n",
        "            D_losses.append(lossD.item())\n",
        "\n",
        "            # Check how the generator is doing by saving G's output on fixed_noise\n",
        "            if (iters % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
        "                with torch.no_grad():\n",
        "                    fake = generator(fixed_noise).detach().cpu()\n",
        "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            iters += 1\n",
        "\n",
        "        print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "              % (epoch+1, num_epochs, lossD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        if epoch%200 == 0:\n",
        "          print(\"Saving Checkpoint...\")\n",
        "          save_checkpoint(epoch)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(G_losses, label=\"G\")\n",
        "    plt.plot(D_losses, label=\"D\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.rcParams['animation.embed_limit'] = 100\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    plt.axis(\"off\")\n",
        "    ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\n",
        "    ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "    HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "ybtYpgwZVO0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gan(ano_mnist_dataset)"
      ],
      "metadata": {
        "id": "GbDMrvAjEFdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "save_models()"
      ],
      "metadata": {
        "id": "wTUdmlCY7g3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d5CPmIqSVO0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def generate_anomaly():\n",
        "    random_idx = random.randint(0, len(test_dataset.data)-1)\n",
        "    img, _ = test_dataset[random_idx]\n",
        "    img = img.numpy()\n",
        "    random_idx = random.randint(4, 20)\n",
        "    img[0][random_idx:random_idx + 3] = np.ones(28, dtype=\"float32\") - 2\n",
        "\n",
        "    return np.expand_dims(img, axis=0)\n",
        "\n",
        "def generate_normals_set(size):\n",
        "    i = torch.from_numpy(np.random.randint(len(test_dataset)-1, size=size))\n",
        "    train_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
        "    data = next(iter(train_loader))[0].numpy()[i]\n",
        "    if len(data.shape) == 3: data = np.expand_dims(data, axis=0)\n",
        "\n",
        "    return torch.from_numpy(data)\n",
        "\n",
        "def generate_anomaly_set(size):\n",
        "    anomalies = generate_anomaly()\n",
        "    for i in range(size-1):\n",
        "        anomalies = np.concatenate((anomalies, generate_anomaly()))\n",
        "\n",
        "    return torch.from_numpy(anomalies)\n",
        "\n",
        "def generate_test_set(size_normals, size_anomalies):\n",
        "    normals = generate_normals_set(size_normals)\n",
        "    anomalies = generate_anomaly_set(size_anomalies)\n",
        "\n",
        "    return normals, anomalies"
      ],
      "metadata": {
        "id": "5Akb2B4sKfaG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_anomaly_score(z, x_query):\n",
        "    lamda = 0.1\n",
        "    g_z = generator(z.to(device))\n",
        "    _, x_prop = discriminator(x_query)\n",
        "    _, g_z_prop = discriminator(g_z)\n",
        "\n",
        "    loss_r = torch.sum(torch.abs(x_query - g_z))\n",
        "    loss_d = torch.sum(torch.abs(x_prop - g_z_prop))\n",
        "\n",
        "    return (1 - lamda) * loss_r + lamda * loss_d"
      ],
      "metadata": {
        "id": "Cfo5Iri0uwzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_inference(query_image, input_type, draw_loss_curve, opt_iterations = 10000, draw_result = True):\n",
        "    z = torch.randn(test_size, size_z, 1, 1, device=device, requires_grad=True).to(device)\n",
        "    z_optimizer = torch.optim.Adam([z], lr=1e-4)\n",
        "    losses = []\n",
        "    score = 0\n",
        "\n",
        "    for i in range(opt_iterations):\n",
        "        loss = get_anomaly_score(z, query_image.unsqueeze(0).to(device))\n",
        "        loss.backward()\n",
        "        z_optimizer.step()\n",
        "        if i % 500==0:\n",
        "            losses.append(loss.data.item())\n",
        "        if i == opt_iterations - 1: score = loss.data.item()\n",
        "\n",
        "    if draw_loss_curve:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.title(\"Loss trajectory during optimization\")\n",
        "        plt.plot(losses, label=\"Loss\")\n",
        "        plt.xlabel(\"iterations\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    if draw_result: draw_results(z, query_image, input_type, score)\n",
        "\n",
        "    if not draw_result: print(f\"Ran Inference with {opt_iterations} iterations. Query-Type was {input_type}. Anonaly-Score: {score}\")\n",
        "    return score\n",
        "\n",
        "def draw_results(z, query_image, input_type, score):\n",
        "    gen_fake = generator(z.to(device))\n",
        "    target = query_image.squeeze().numpy()\n",
        "    img = gen_fake.detach().numpy().squeeze()\n",
        "\n",
        "    num_rows = 1\n",
        "    images_in_row = 4\n",
        "    f, ax = plt.subplots(num_rows, images_in_row, figsize=(10,4))\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 1)\n",
        "    plt.imshow(target, cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Query Image\")\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 2)\n",
        "    plt.imshow(img, cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"G(z_Gamma)\")\n",
        "\n",
        "    diff = np.abs(img - target)*255\n",
        "    diff_isclose = np.isclose(img, target, rtol=1.2, atol=0.4)\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 3)\n",
        "    plt.imshow(Image.fromarray(diff), cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"|G(z_Gamma) - x|\")\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 4)\n",
        "    plt.imshow(Image.fromarray(diff_isclose), cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"np.isclose()\")\n",
        "\n",
        "    f.suptitle(f'Sample Type: {input_type} - Score: {score}', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "def create_roc_curve(file_path):\n",
        "    data = pd.read_csv(file_path, delimiter=';').iloc[: , 1:].to_numpy()\n",
        "    y_label, y_scores = np.hsplit(data, 2)\n",
        "    y_label = y_label.reshape(len(y_label))\n",
        "    y_scores = np.array(y_scores.reshape(len(y_scores)), dtype='float64')\n",
        "    label_dict = {'normal': 0, 'anomaly': 1}\n",
        "    y_label = np.array([label_dict[y_i] for y_i in y_label])\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_label, y_scores)\n",
        "    auc_value = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
        "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "    plt.title(f\"ROC Curve - AUC: {auc_value}\", fontsize=18)\n",
        "    plt.xlabel(\"FPR\", fontsize=14)\n",
        "    plt.ylabel(\"TPR\", fontsize=14)\n",
        "    plt.legend()\n",
        "\n",
        "def test_inference(size_normals, size_anomalies, draw_loss_curve = False, save_statistics = True, draw_result = True, opt_iterations = 10000):\n",
        "    statistics = {\n",
        "        \"sample-type\": [],\n",
        "        \"score\": []\n",
        "    }\n",
        "\n",
        "    normals, anomalies = generate_test_set(size_normals, size_anomalies)\n",
        "    for n in normals:\n",
        "        score = run_inference(n, 'NORMAL', draw_loss_curve=draw_loss_curve, opt_iterations = opt_iterations, draw_result = draw_result)\n",
        "        if save_statistics:\n",
        "            statistics[\"sample-type\"].append(\"normal\")\n",
        "            statistics[\"score\"].append(round(score, 2))\n",
        "    for a in anomalies:\n",
        "        score = run_inference(a, 'ANOMALY', draw_loss_curve=draw_loss_curve, opt_iterations = opt_iterations, draw_result = draw_result)\n",
        "        if save_statistics:\n",
        "            statistics[\"sample-type\"].append(\"anomaly\")\n",
        "            statistics[\"score\"].append(round(score, 2))\n",
        "    if save_statistics:\n",
        "        timestr = time.strftime('%d %b %Y %H:%M:%S GMT', time.localtime())\n",
        "        df = pd.DataFrame(statistics)\n",
        "        df.to_csv(f'./data/statistics/stat_{timestr}.csv', sep=';')\n"
      ],
      "metadata": {
        "id": "ia_ly0qkuwzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load_models()\n",
        "# test_inference(80, 20, draw_loss_curve = False, save_statistics = True, draw_result = True, opt_iterations = 10000)"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "grnbE4sBuwzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# file = './data/statistics/stat_02 Apr 2023 17:30:50 GMT.csv'\n",
        "# create_roc_curve(file)"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "FHXpEBZguwzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_gan(ano_mnist_dataset)"
      ],
      "metadata": {
        "id": "Qwt6o8I11h4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# save_models()"
      ],
      "metadata": {
        "id": "s-4EyjuTVO0l"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}