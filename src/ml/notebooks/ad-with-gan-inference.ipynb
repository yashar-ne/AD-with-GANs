{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "3dlxtjAuDfKX",
        "outputId": "ca4585aa-dd65-449b-bf49-ce6155c93286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "xtoXvC1nfhOK",
        "ExecuteTime": {
          "end_time": "2023-06-04T13:03:44.407877828Z",
          "start_time": "2023-06-04T13:03:42.796806718Z"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.utils as vutils\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "import csv\n",
        "import math\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "kwb65Q9DfhOO",
        "ExecuteTime": {
          "end_time": "2023-06-04T13:03:45.180199136Z",
          "start_time": "2023-06-04T13:03:45.157605225Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, size_z, num_feature_maps, num_color_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.size_z = size_z\n",
        "        self.network = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.size_z, num_feature_maps * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps * 4, num_feature_maps * 2, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps * 2, num_feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps, num_color_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.network(x)\n",
        "        return output\n",
        "\n",
        "    def gen_shifted(self, x, shift):\n",
        "        shift = torch.unsqueeze(shift, -1)\n",
        "        shift = torch.unsqueeze(shift, -1)\n",
        "        return self.forward(x + shift)\n"
      ],
      "metadata": {
        "id": "wNc602dbgXO3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_feature_maps, num_color_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(num_color_channels, num_feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps, num_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps * 2, num_feature_maps * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        feature = out\n",
        "        out = self.fc(out)\n",
        "        return out.view(-1, 1).squeeze(1), feature"
      ],
      "metadata": {
        "id": "5bkWt3mZgbvB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnoMNIST(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        root_dir = os.path.join(root_dir, \"AnoMNIST\")\n",
        "        assert os.path.exists(os.path.join(root_dir, \"anomnist_dataset.csv\")), \"Invalid root directory\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label = pd.read_csv(os.path.join(root_dir, \"anomnist_dataset.csv\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.label.iloc[idx, 0])\n",
        "        image_label = {\"label\": self.label.iloc[idx, 1], \"anomaly\": self.label.iloc[idx, 2]}\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, image_label\n",
        "\n",
        "\n",
        "class AnomalyExtendedMNIST(datasets.MNIST):\n",
        "    def __getitem__(self, idx):\n",
        "        return super(AnomalyExtendedMNIST, self).__getitem__(idx)[0], {\"label\": super(AnomalyExtendedMNIST, self).__getitem__(idx)[1], \"anomaly\": False}"
      ],
      "metadata": {
        "id": "gdZT3IEJg2V3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnoClassMNIST(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        root_dir = os.path.join(root_dir, \"AnoClassMNIST\")\n",
        "        assert os.path.exists(os.path.join(root_dir, \"ano_class_mnist_dataset.csv\")), \"Invalid root directory\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label = pd.read_csv(os.path.join(root_dir, \"ano_class_mnist_dataset.csv\"))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.label.iloc[idx, 0])\n",
        "        image_label = {\"label\": self.label.iloc[idx, 1], \"anomaly\": self.label.iloc[idx, 2]}\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, image_label\n",
        "\n",
        "\n",
        "class AnomalyExtendedMNIST(datasets.MNIST):\n",
        "    def __getitem__(self, idx):\n",
        "        return super(AnomalyExtendedMNIST, self).__getitem__(idx)[0], {\"label\": super(AnomalyExtendedMNIST, self).__getitem__(idx)[1], \"anomaly\": False}"
      ],
      "metadata": {
        "id": "LezYoeYcx3UQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lined_mnist_images(base_folder, num, max_augmentation_thickness=5,\n",
        "                                randomize_augmentation_thickness=False, labels=[]):\n",
        "    assert max_augmentation_thickness <= 7, \"max_augmentation_thickness must be smaller than 7\"\n",
        "    os.makedirs(base_folder, exist_ok=True)\n",
        "\n",
        "    dataset = datasets.MNIST(\n",
        "        root=base_folder,\n",
        "        train=True,\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    if len(labels) > 0:\n",
        "        dataset = [d for d in dataset if (d[1] in labels)]\n",
        "    else:\n",
        "        dataset = dataset.data\n",
        "\n",
        "    ano_mnist_drop_folder = os.path.join(base_folder, \"AnoMNIST\")\n",
        "    csv_path = os.path.join(ano_mnist_drop_folder, \"anomnist_dataset.csv\")\n",
        "\n",
        "    os.makedirs(base_folder, exist_ok=True)\n",
        "    os.makedirs(ano_mnist_drop_folder, exist_ok=True)\n",
        "\n",
        "    augmentation_thickness: int = random.randint(1, max_augmentation_thickness)\n",
        "    for i in range(num):\n",
        "        random_idx = random.randint(0, len(dataset) - 1)\n",
        "        img, label = dataset[random_idx]\n",
        "\n",
        "        augmentation_thickness = random.randint(3,\n",
        "                                                max_augmentation_thickness) if randomize_augmentation_thickness else augmentation_thickness\n",
        "        random_idx = random.randint(4, 20)\n",
        "        for j in range(img.size[0]):\n",
        "            for k in range(augmentation_thickness):\n",
        "                img.putpixel((j, random_idx + k + 1), 0)\n",
        "\n",
        "        img.save(os.path.join(ano_mnist_drop_folder, f\"img_aug_{label}_{i}.png\"))\n",
        "        with open(csv_path, 'a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            fields = [f'img_aug_{label}_{i}.png', f\"{label}\", \"True\"]\n",
        "            writer.writerow(fields)\n",
        "\n",
        "\n",
        "def generate_anomalous_image_files(base_folder, num, labels=[], copy_zip_to=''):\n",
        "    if os.path.exists(base_folder):\n",
        "        shutil.rmtree(base_folder)\n",
        "\n",
        "    ano_mnist_drop_folder = os.path.join(base_folder, \"AnoMNIST\")\n",
        "    csv_path = os.path.join(ano_mnist_drop_folder, \"anomnist_dataset.csv\")\n",
        "\n",
        "    os.makedirs(base_folder, exist_ok=True)\n",
        "    os.makedirs(ano_mnist_drop_folder, exist_ok=True)\n",
        "\n",
        "    with open(csv_path, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        fields = [\"filename\", \"label\", \"anomaly\"]\n",
        "        writer.writerow(fields)\n",
        "\n",
        "    generate_lined_mnist_images(base_folder, num=num, labels=labels)\n",
        "    if copy_zip_to:\n",
        "      shutil.make_archive(os.path.join(copy_zip_to, \"AnoMNIST\"), 'zip', ano_mnist_drop_folder)\n",
        "\n",
        "def get_ano_mnist_dataset(transform, root_dir, labels=[9], train_size=0.9):\n",
        "    ano_mnist_dataset = AnoMNIST(\n",
        "        root_dir=root_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    mnist_dataset = AnomalyExtendedMNIST(\n",
        "        root=root_dir,\n",
        "        train=True,\n",
        "        transform=transform,\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    dat = torch.utils.data.ConcatDataset([ano_mnist_dataset, mnist_dataset])\n",
        "\n",
        "    if len(labels) > 0:\n",
        "        dat = [d for d in dat if (d[1]['label'] in labels)]\n",
        "\n",
        "    absolute_train_size = int(len(dat) * train_size)\n",
        "    absolute_test_size = len(dat) - absolute_train_size\n",
        "    return torch.utils.data.random_split(dat, [absolute_train_size, absolute_test_size])\n",
        "\n",
        "\n",
        "def generate_ano_class_mnist_dataset(root_dir, norm_class=9, ano_class=6, ano_fraction=0.2, copy_zip_to='/content/drive/MyDrive/Colab/data'):\n",
        "    if os.path.exists(root_dir):\n",
        "        shutil.rmtree(root_dir)\n",
        "\n",
        "    ano_mnist_drop_folder = os.path.join(root_dir, \"AnoClassMNIST\")\n",
        "    csv_path = os.path.join(ano_mnist_drop_folder, \"ano_class_mnist_dataset.csv\")\n",
        "\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    os.makedirs(ano_mnist_drop_folder, exist_ok=True)\n",
        "\n",
        "    with open(csv_path, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        fields = [\"filename\", \"label\", \"anomaly\"]\n",
        "        writer.writerow(fields)\n",
        "\n",
        "    mnist_dataset = MNIST(\n",
        "        root=root_dir,\n",
        "        train=True,\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    norms = [d for d in mnist_dataset if (d[1] == norm_class)]\n",
        "    for i, img in enumerate(norms):\n",
        "        img[0].save(os.path.join(ano_mnist_drop_folder, f\"img_{norm_class}_{i}.png\"))\n",
        "        with open(csv_path, 'a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            fields = [f'img_aug_{norm_class}_{i}.png', f\"{norm_class}\", \"False\"]\n",
        "            writer.writerow(fields)\n",
        "\n",
        "\n",
        "    anos = [d for d in mnist_dataset if (d[1] == ano_class)]\n",
        "    for i, img in enumerate(anos):\n",
        "        img[0].save(os.path.join(ano_mnist_drop_folder, f\"img_{ano_class}_{i}.png\"))\n",
        "        with open(csv_path, 'a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            fields = [f'img_aug_{ano_class}_{i}.png', f\"{ano_class}\", \"True\"]\n",
        "            writer.writerow(fields)\n",
        "\n",
        "    if copy_zip_to:\n",
        "      shutil.make_archive(os.path.join(copy_zip_to, \"AnoClassMNIST\"), 'zip', ano_mnist_drop_folder)\n",
        "\n",
        "\n",
        "def get_ano_class_mnist_dataset(root_dir, norm_class=9, ano_class=6, ano_fraction=0.2):\n",
        "    mnist_dataset = MNIST(\n",
        "        root=root_dir,\n",
        "        train=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(.5,), std=(.5,))\n",
        "        ]),\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    norms = [d for d in mnist_dataset if (d[1] == norm_class)]\n",
        "    anos = [d for d in mnist_dataset if (d[1] == ano_class)]\n",
        "\n",
        "    return torch.utils.data.ConcatDataset([norms, anos[:round(ano_fraction*len(anos))]])"
      ],
      "metadata": {
        "id": "a5t0c6FqgiK4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_ano_class_mnist_dataset('/content/data', norm_class=9, ano_class=6, ano_fraction=0.2, copy_zip_to='/content/drive/MyDrive/Colab/data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TljMlX_GyGUB",
        "outputId": "bce0313e-314c-47fe-ffcc-357b7f8183dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 239639128.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/train-images-idx3-ubyte.gz to /content/data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 23855000.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/train-labels-idx1-ubyte.gz to /content/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 85532376.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16725661.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ano_mnist_from_drive(drop_folder):\n",
        "  with zipfile.ZipFile('/content/drive/MyDrive/Colab/data/AnoMNIST.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(drop_folder)"
      ],
      "metadata": {
        "id": "CovsspsjqzjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "num_classes = 1\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 200\n",
        "num_color_channels = 1\n",
        "num_feature_maps_g = 64\n",
        "num_feature_maps_d = 64\n",
        "size_z = 100\n",
        "adam_beta1 = 0.1\n",
        "test_size = 1"
      ],
      "metadata": {
        "id": "1Y3il4ovfhOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "generator = Generator(size_z=size_z,\n",
        "                      num_feature_maps=num_feature_maps_g,\n",
        "                      num_color_channels=num_color_channels).to(device)\n",
        "discriminator = Discriminator(num_feature_maps=num_feature_maps_d,\n",
        "                              num_color_channels=num_color_channels).to(device)"
      ],
      "metadata": {
        "id": "tCj3teR7fhOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(.5,), std=(.5,))\n",
        "])\n",
        "\n",
        "#generate_anomalous_image_files(base_folder='/content/data', num=2000, labels=[9], copy_zip_to='/content/drive/MyDrive/Colab/data') # number of normals is: 5949\n",
        "#load_ano_mnist_from_drive(drop_folder='/content/data')\n",
        "#ano_mnist_dataset, _ = get_ano_mnist_dataset(transform=transform, root_dir='/content/data', labels=[9])\n",
        "dataset = get_ano_mnist_dataset(transform=transform, root_dir='/content/data', labels=[9])"
      ],
      "metadata": {
        "id": "VlILqAhFfhOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save/Load models into generator and discriminator models"
      ],
      "metadata": {
        "collapsed": false,
        "id": "nNUq7iL5VO0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch):\n",
        "    print(\"Saving Checkpoint...\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    timestamp = time.time()\n",
        "    torch.save(generator.state_dict(),f'/content/drive/My Drive/Colab/saved_models/generator_epoch_{epoch}_{timestamp}.pkl')\n",
        "    torch.save(discriminator.state_dict(),f'/content/drive/My Drive/Colab/saved_models/discriminator_epoch_{epoch}_{timestamp}.pkl')\n",
        "\n",
        "def save_models():\n",
        "    print(\"Saving Models...\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    torch.save(generator.state_dict(),f'/content/drive/My Drive/Colab/saved_models/generator_latest.pkl')\n",
        "    torch.save(discriminator.state_dict(),f'/content/drive/My Drive/Colab/saved_models/discriminator_latest.pkl')\n",
        "\n",
        "def load_models():\n",
        "    print(\"Loading Models...\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    generator.load_state_dict(torch.load(\"/content/drive/My Drive/Colab/saved_models/generator.pkl\", map_location=torch.device(device)))\n",
        "    discriminator.load_state_dict(torch.load('/content/drive/My Drive/Colab/saved_models/discriminator.pkl', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "AUkBkw0LdQCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the GAN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Mr8Pn8i9VO0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(64, size_z, 1, 1, device=device)\n",
        "\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=learning_rate, betas=(adam_beta1, 0.999))\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(adam_beta1, 0.999))\n",
        "\n",
        "def train_gan(dataset):\n",
        "    img_list = []\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "    iters = 0\n",
        "    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "\n",
        "    print(\"Starting Training Loop...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (real_images, _) in enumerate(dataloader):\n",
        "            # get batch-size from actual image batch\n",
        "            bs = real_images.shape[0]\n",
        "\n",
        "            # -- train discriminator --\n",
        "\n",
        "            # reset/clear discriminators gradient\n",
        "            discriminator.zero_grad()\n",
        "\n",
        "            # move images to either CPU or GPU\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # creates a label tensor filled with 1s\n",
        "            label = torch.full((bs,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "            # get probs for discriminators guess on the real images\n",
        "            output, _ = discriminator(real_images)\n",
        "\n",
        "            # get loss for real images. that means it calculates the difference\n",
        "            # between the output of the model with the current parameter and the\n",
        "            # target (goal) of what the model is supposed to do\n",
        "            # output --> current outcome of the model\n",
        "            # label  --> target of the model\n",
        "            lossD_real = criterion(output, label)\n",
        "\n",
        "            # calculates the gradient (using chain-rule)\n",
        "            # see https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
        "            lossD_real.backward()\n",
        "\n",
        "            # Gets the mean value of all results from the discriminator to get an average\n",
        "            # probability of all sample evaluations (for real data ) --> D(x)\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            # create noise as an input for the G in order to create fake images\n",
        "            noise = torch.randn(bs, size_z, 1, 1, device=device)\n",
        "\n",
        "            # use generator to map input noise to an output that is supposed do become fake images during training\n",
        "            fake_images = generator(noise)\n",
        "\n",
        "            # creates a label tensor filled with 0s\n",
        "            label.fill_(fake_label)\n",
        "\n",
        "            # get discriminators guess on fake images\n",
        "            output, _ = discriminator(fake_images.detach())\n",
        "\n",
        "            # get loss for fake images\n",
        "            lossD_fake = criterion(output, label)\n",
        "\n",
        "            # adjust parameter to identify fakes\n",
        "            lossD_fake.backward()\n",
        "\n",
        "            # gets the mean value of all results from the discriminator to get an average\n",
        "            # probability of all sample evaluations. this time for the fake images that were\n",
        "            # generated by the generator --> D(G(z))\n",
        "            D_G_z1 = output.mean().item()\n",
        "\n",
        "            # calculate loss\n",
        "            lossD = lossD_real + lossD_fake\n",
        "\n",
        "            # adjust models (discriminator) parameter\n",
        "            optimizerD.step()\n",
        "\n",
        "            # -- train generator --\n",
        "\n",
        "            # reset/clear generators gradient\n",
        "            generator.zero_grad()\n",
        "\n",
        "            # creates a label tensor filled with 1s\n",
        "            label.fill_(real_label)\n",
        "\n",
        "            # get discriminators guess on fake images\n",
        "            output, _ = discriminator(fake_images)\n",
        "            output = output.view(-1)\n",
        "\n",
        "            # get loss for fake images\n",
        "            lossG = criterion(output, label)\n",
        "\n",
        "            # adjust parameter to generate fakes\n",
        "            lossG.backward()\n",
        "\n",
        "            # gets the mean value of all results from the discriminator to get an average\n",
        "            # probability of all sample evaluations. this time for the fake images that were\n",
        "            # generated by the generator --> D(G(z))\n",
        "            D_G_z2 = output.mean().item()\n",
        "\n",
        "            # adjust models (generator) parameter\n",
        "            optimizerG.step()\n",
        "            # Save Losses for plotting later\n",
        "            G_losses.append(lossG.item())\n",
        "            D_losses.append(lossD.item())\n",
        "\n",
        "            # Check how the generator is doing by saving G's output on fixed_noise\n",
        "            if (iters % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
        "                with torch.no_grad():\n",
        "                    fake = generator(fixed_noise).detach().cpu()\n",
        "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            iters += 1\n",
        "\n",
        "        print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "              % (epoch+1, num_epochs, lossD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        if epoch>0 and epoch%50 == 0:\n",
        "          save_checkpoint(epoch)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(G_losses, label=\"G\")\n",
        "    plt.plot(D_losses, label=\"D\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.rcParams['animation.embed_limit'] = 100\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    plt.axis(\"off\")\n",
        "    ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\n",
        "    ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "    HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "ybtYpgwZVO0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_gan(ano_mnist_dataset)\n",
        "# data = get_ano_class_mnist_dataset(root_dir='/content/drive/MyDrive/Colab/data', ano_fraction=0.2)\n",
        "train_gan(data)"
      ],
      "metadata": {
        "id": "GbDMrvAjEFdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Models...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "save_models()"
      ],
      "metadata": {
        "id": "wTUdmlCY7g3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcc8e8e-c4a1-446f-fc70-d0f93eed185d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d5CPmIqSVO0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def generate_anomaly():\n",
        "    random_idx = random.randint(0, len(test_dataset.data)-1)\n",
        "    img, _ = test_dataset[random_idx]\n",
        "    img = img.numpy()\n",
        "    random_idx = random.randint(4, 20)\n",
        "    img[0][random_idx:random_idx + 3] = np.ones(28, dtype=\"float32\") - 2\n",
        "\n",
        "    return np.expand_dims(img, axis=0)\n",
        "\n",
        "def generate_normals_set(size):\n",
        "    i = torch.from_numpy(np.random.randint(len(test_dataset)-1, size=size))\n",
        "    train_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
        "    data = next(iter(train_loader))[0].numpy()[i]\n",
        "    if len(data.shape) == 3: data = np.expand_dims(data, axis=0)\n",
        "\n",
        "    return torch.from_numpy(data)\n",
        "\n",
        "def generate_anomaly_set(size):\n",
        "    anomalies = generate_anomaly()\n",
        "    for i in range(size-1):\n",
        "        anomalies = np.concatenate((anomalies, generate_anomaly()))\n",
        "\n",
        "    return torch.from_numpy(anomalies)\n",
        "\n",
        "def generate_test_set(size_normals, size_anomalies):\n",
        "    normals = generate_normals_set(size_normals)\n",
        "    anomalies = generate_anomaly_set(size_anomalies)\n",
        "\n",
        "    return normals, anomalies"
      ],
      "metadata": {
        "id": "5Akb2B4sKfaG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_anomaly_score(z, x_query):\n",
        "    lamda = 0.1\n",
        "    g_z = generator(z.to(device))\n",
        "    _, x_prop = discriminator(x_query)\n",
        "    _, g_z_prop = discriminator(g_z)\n",
        "\n",
        "    loss_r = torch.sum(torch.abs(x_query - g_z))\n",
        "    loss_d = torch.sum(torch.abs(x_prop - g_z_prop))\n",
        "\n",
        "    return (1 - lamda) * loss_r + lamda * loss_d"
      ],
      "metadata": {
        "id": "Cfo5Iri0uwzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_inference(query_image, input_type, draw_loss_curve, opt_iterations = 10000, draw_result = True):\n",
        "    z = torch.randn(test_size, size_z, 1, 1, device=device, requires_grad=True).to(device)\n",
        "    z_optimizer = torch.optim.Adam([z], lr=1e-4)\n",
        "    losses = []\n",
        "    score = 0\n",
        "\n",
        "    for i in range(opt_iterations):\n",
        "        loss = get_anomaly_score(z, query_image.unsqueeze(0).to(device))\n",
        "        loss.backward()\n",
        "        z_optimizer.step()\n",
        "        if i % 500==0:\n",
        "            losses.append(loss.data.item())\n",
        "        if i == opt_iterations - 1: score = loss.data.item()\n",
        "\n",
        "    if draw_loss_curve:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.title(\"Loss trajectory during optimization\")\n",
        "        plt.plot(losses, label=\"Loss\")\n",
        "        plt.xlabel(\"iterations\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    if draw_result: draw_results(z, query_image, input_type, score)\n",
        "\n",
        "    if not draw_result: print(f\"Ran Inference with {opt_iterations} iterations. Query-Type was {input_type}. Anonaly-Score: {score}\")\n",
        "    return score\n",
        "\n",
        "def draw_results(z, query_image, input_type, score):\n",
        "    gen_fake = generator(z.to(device))\n",
        "    target = query_image.squeeze().numpy()\n",
        "    img = gen_fake.detach().numpy().squeeze()\n",
        "\n",
        "    num_rows = 1\n",
        "    images_in_row = 4\n",
        "    f, ax = plt.subplots(num_rows, images_in_row, figsize=(10,4))\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 1)\n",
        "    plt.imshow(target, cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Query Image\")\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 2)\n",
        "    plt.imshow(img, cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"G(z_Gamma)\")\n",
        "\n",
        "    diff = np.abs(img - target)*255\n",
        "    diff_isclose = np.isclose(img, target, rtol=1.2, atol=0.4)\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 3)\n",
        "    plt.imshow(Image.fromarray(diff), cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"|G(z_Gamma) - x|\")\n",
        "\n",
        "    f.add_subplot(num_rows, images_in_row, 4)\n",
        "    plt.imshow(Image.fromarray(diff_isclose), cmap=\"binary\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"np.isclose()\")\n",
        "\n",
        "    f.suptitle(f'Sample Type: {input_type} - Score: {score}', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "def create_roc_curve(file_path):\n",
        "    data = pd.read_csv(file_path, delimiter=';').iloc[: , 1:].to_numpy()\n",
        "    y_label, y_scores = np.hsplit(data, 2)\n",
        "    y_label = y_label.reshape(len(y_label))\n",
        "    y_scores = np.array(y_scores.reshape(len(y_scores)), dtype='float64')\n",
        "    label_dict = {'normal': 0, 'anomaly': 1}\n",
        "    y_label = np.array([label_dict[y_i] for y_i in y_label])\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_label, y_scores)\n",
        "    auc_value = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
        "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "    plt.title(f\"ROC Curve - AUC: {auc_value}\", fontsize=18)\n",
        "    plt.xlabel(\"FPR\", fontsize=14)\n",
        "    plt.ylabel(\"TPR\", fontsize=14)\n",
        "    plt.legend()\n",
        "\n",
        "def test_inference(size_normals, size_anomalies, draw_loss_curve = False, save_statistics = True, draw_result = True, opt_iterations = 10000):\n",
        "    statistics = {\n",
        "        \"sample-type\": [],\n",
        "        \"score\": []\n",
        "    }\n",
        "\n",
        "    normals, anomalies = generate_test_set(size_normals, size_anomalies)\n",
        "    for n in normals:\n",
        "        score = run_inference(n, 'NORMAL', draw_loss_curve=draw_loss_curve, opt_iterations = opt_iterations, draw_result = draw_result)\n",
        "        if save_statistics:\n",
        "            statistics[\"sample-type\"].append(\"normal\")\n",
        "            statistics[\"score\"].append(round(score, 2))\n",
        "    for a in anomalies:\n",
        "        score = run_inference(a, 'ANOMALY', draw_loss_curve=draw_loss_curve, opt_iterations = opt_iterations, draw_result = draw_result)\n",
        "        if save_statistics:\n",
        "            statistics[\"sample-type\"].append(\"anomaly\")\n",
        "            statistics[\"score\"].append(round(score, 2))\n",
        "    if save_statistics:\n",
        "        timestr = time.strftime('%d %b %Y %H:%M:%S GMT', time.localtime())\n",
        "        df = pd.DataFrame(statistics)\n",
        "        df.to_csv(f'./data/statistics/stat_{timestr}.csv', sep=';')\n"
      ],
      "metadata": {
        "id": "ia_ly0qkuwzD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load_models()\n",
        "# test_inference(80, 20, draw_loss_curve = False, save_statistics = True, draw_result = True, opt_iterations = 10000)"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "grnbE4sBuwzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# file = './data/statistics/stat_02 Apr 2023 17:30:50 GMT.csv'\n",
        "# create_roc_curve(file)"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "FHXpEBZguwzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_gan(ano_mnist_dataset)"
      ],
      "metadata": {
        "id": "Qwt6o8I11h4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# save_models()"
      ],
      "metadata": {
        "id": "s-4EyjuTVO0l"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}