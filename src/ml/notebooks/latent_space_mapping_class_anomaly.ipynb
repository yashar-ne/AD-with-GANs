{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "9rCsmPpNzrBo",
        "outputId": "491e4678-0c4d-4ae6-c8ac-251dc2faa9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2023-06-19T13:16:49.352615778Z",
          "start_time": "2023-06-19T13:16:49.272631925Z"
        },
        "id": "uKhESdEpzmea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import csv\n",
        "import zipfile\n",
        "import PIL\n",
        "import math\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, size_z, num_feature_maps, num_color_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.size_z = size_z\n",
        "        self.network = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.size_z, num_feature_maps * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps * 4, num_feature_maps * 2, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps * 2, num_feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(num_feature_maps, num_color_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.network(x)\n",
        "        return output\n",
        "\n",
        "    def gen_shifted(self, x, shift):\n",
        "        shift = torch.unsqueeze(shift, -1)\n",
        "        shift = torch.unsqueeze(shift, -1)\n",
        "        return self.forward(x + shift)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-19T13:16:50.491940517Z",
          "start_time": "2023-06-19T13:16:50.476819472Z"
        },
        "id": "AZKotCEvzmee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_feature_maps, num_color_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(num_color_channels, num_feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps, num_feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps * 2, num_feature_maps * 4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(num_feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(num_feature_maps * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        feature = out\n",
        "        out = self.fc(out)\n",
        "        return out.view(-1, 1).squeeze(1), feature"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-19T13:16:51.402689357Z",
          "start_time": "2023-06-19T13:16:51.383963387Z"
        },
        "id": "9jo7Zrbczmeh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "class LatentSpaceMapper:\n",
        "    def __init__(self, generator: Generator, discriminator: Discriminator, device):\n",
        "        self.generator: Generator = generator\n",
        "        self.generator.to(device)\n",
        "        self.discriminator: Discriminator = discriminator\n",
        "        self.discriminator.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def map_image_to_point_in_latent_space(self, image: torch.Tensor, batch_size=1, size_z=100, max_opt_iterations=30000, opt_threshold=140.0, plateu_threshold=3.0, check_every_n_iter=4000, learning_rate=0.4, print_every_n_iters=10000, ignore_rules_below_threshold=50, retry_after_n_iters=10000, immediate_retry_threshold=200):\n",
        "        image.to(self.device)\n",
        "        z = torch.randn(batch_size, size_z, 1, 1, device=self.device, requires_grad=True)\n",
        "        z_optimizer = torch.optim.Adam([z], lr=learning_rate)\n",
        "        losses = []\n",
        "        final_loss = 0\n",
        "        latest_checkpoint_loss = 0\n",
        "\n",
        "        # scheduler = lr_scheduler.LinearLR(z_optimizer, start_factor=0.4, end_factor=0.001, total_iters=max_opt_iterations-(math.floor(max_opt_iterations*0.2)))\n",
        "        # scheduler = lr_scheduler.StepLR(z_optimizer, step_size=max_opt_iterations, gamma=0.9)\n",
        "        # scheduler = torch.optim.lr_scheduler.CyclicLR(z_optimizer, base_lr=0.01, max_lr=0.4, cycle_momentum=False)\n",
        "        for i in range(max_opt_iterations):\n",
        "            retry = False\n",
        "            loss = self.__get_anomaly_score(z, image.unsqueeze(0).to(self.device))\n",
        "            loss.backward()\n",
        "            z_optimizer.step()\n",
        "            final_loss = loss.data.item()\n",
        "\n",
        "            if i == 1:\n",
        "                latest_checkpoint_loss = loss.data.item()\n",
        "\n",
        "            if loss.data.item() < opt_threshold*batch_size:\n",
        "                print(f\"Iteration: {i} -- Reached Defined Optimum -- Final Loss: {loss.data.item()}\")\n",
        "                break\n",
        "\n",
        "            if (i % print_every_n_iters == 0 and i != 0) or (i == max_opt_iterations-1):\n",
        "                print(f\"Iteration: {i} -- Current Loss: {loss.data.item()} -- Current Learning-Rate: {z_optimizer.param_groups[0]['lr']}\")\n",
        "                losses.append(loss.data.item())\n",
        "\n",
        "            if i % check_every_n_iter == 0 and i != 0:\n",
        "                if abs(loss.data.item()-latest_checkpoint_loss) < plateu_threshold:\n",
        "                    print(f\"Reached Plateu at Iteration {i} -- Loss: {loss.data.item()}\")\n",
        "                    retry = True\n",
        "                    break\n",
        "                if loss.data.item() > immediate_retry_threshold:\n",
        "                    print(f\"Loss at Iteration {i} too high -- Loss: {loss.data.item()}\")\n",
        "                    retry = True\n",
        "                    break\n",
        "                latest_checkpoint_loss = loss.data.item()\n",
        "\n",
        "            if i == retry_after_n_iters and loss.data.item() > ignore_rules_below_threshold:\n",
        "                retry = True\n",
        "                break\n",
        "\n",
        "            #scheduler.step()\n",
        "\n",
        "        return z, final_loss, retry\n",
        "\n",
        "    def __get_anomaly_score(self, z, x_query):\n",
        "        lamda = 0.1\n",
        "        g_z = self.generator(z.to(self.device))\n",
        "        loss_r = torch.sum(torch.abs(x_query - g_z))\n",
        "\n",
        "        return loss_r\n",
        "\n",
        "        #_, x_prop = self.discriminator(x_query)\n",
        "        #_, g_z_prop = self.discriminator(g_z)\n",
        "        #loss_d = torch.sum(torch.abs(x_prop - g_z_prop))\n",
        "        #return (1 - lamda) * loss_r + lamda * loss_d\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-19T13:16:52.168904698Z",
          "start_time": "2023-06-19T13:16:52.152945768Z"
        },
        "id": "bDlGQeeozmej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_color_channels = 1\n",
        "num_feature_maps_g = 64\n",
        "num_feature_maps_d = 64\n",
        "size_z = 100\n",
        "\n",
        "device"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-19T13:16:56.718929255Z",
          "start_time": "2023-06-19T13:16:56.701170727Z"
        },
        "id": "sy0CAfU_zmep",
        "outputId": "1feaf3d1-19d2-42d2-f553-170821388104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "generator = Generator(size_z=size_z,\n",
        "                      num_feature_maps=num_feature_maps_g,\n",
        "                      num_color_channels=num_color_channels).to(device)\n",
        "discriminator = Discriminator(num_feature_maps=num_feature_maps_d,\n",
        "                              num_color_channels=num_color_channels).to(device)\n",
        "\n",
        "generator.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab/saved_models/generator.pkl\", map_location=torch.device(device)))\n",
        "discriminator.load_state_dict(torch.load('/content/drive/MyDrive/Colab/saved_models/discriminator.pkl', map_location=torch.device(device)))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-19T13:16:58.039609982Z",
          "start_time": "2023-06-19T13:16:57.998343596Z"
        },
        "id": "IJj0sB2Izmeq",
        "outputId": "2facd3bb-3832-4a34-d8c5-84f390bfc178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "def get_ano_class_mnist_dataset(root_dir, norm_class=9, ano_class=6, ano_fraction=0.1):\n",
        "    mnist_dataset = MNIST(\n",
        "        root=root_dir,\n",
        "        train=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(.5,), std=(.5,))\n",
        "        ]),\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    norms = [d for d in mnist_dataset if (d[1] == norm_class)]\n",
        "    anos = [d for d in mnist_dataset if (d[1] == ano_class)]\n",
        "\n",
        "    dataset = torch.utils.data.ConcatDataset([norms, anos[:round(ano_fraction*len(anos))]])\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "data = get_ano_class_mnist_dataset(root_dir='/content/drive/MyDrive/Colab/data')"
      ],
      "metadata": {
        "id": "BRc3Fm41zmes"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6541 images left\n",
            "Iteration: 5000 -- Current Loss: 57.509796142578125 -- Current Learning-Rate: 0.1\n",
            "Iteration: 10000 -- Current Loss: 50.333702087402344 -- Current Learning-Rate: 0.1\n",
            "Iteration: 10652 -- Reached Defined Optimum -- Final Loss: 49.99961471557617\n",
            "Original Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAACPElEQVR4nO3ZvWoUYRTG8d9qQCuLgGInSLCwsrDTQvADQYTY+FVIzAWIiJX3YGclNrHSxgsQ1ChWFmJh4QYCaiFBJNqJIRiLnVlm2Vk3MxM4zTnFMHN25ux/nnk4877v9HpiY1fw/ydAAiRAAiRAAiRAAiRAAsQDzHQtcBCe4gKslyU/4idcgbXJ14crkAC9DlOzU7AJz6vpmWEabsPDyUXCFUiAcIC2jWgJ16i4bRgL8Kg8mp1SKFyBBAgHaGHC13BoJLWMZ7CC942qhSuQAA09sKR4/AfKzCxcxuMys0Fdf5oQ4QokQDhAiyHZRrlzBt6M/f6Xign3DlP1Ea5AAjRsRFsqj/cSdXewWW4+w3F4N7liuAIJEA7QsBFt4Xd5MDoJNJbeA7unVAxXIAEaeuAV+nDD/z3wVjFH+zqlYrgCCRAO0GJEdAzuK+CHg52L8ENhwm8GjnVzSrVwBRIgHKDF+sAHOD2W/gO9suS2byxcgQToslqOYurVV4x/9hk0I32c3c714QokQDhAZxN+gf3VzDkGQ7JtRbgCCdD2i8k8blH0H4rVgFV8b1QoXIEECAdoYcJ5WMRJKnPDVYqxeaMIVyABGnpgAXfhSHntGrwwfQ42IcIVSIBwgBoTXoU5xWttpUyvw3UcZtB/zsNReNAeIFyBBAgHqDHhHNwbO2t0SXQRL5WbDhGuQAI0fBsu4w782imAcAUSIBygxoQn6s/8BE/KnR2LcAUSoPMaUdcIVyABEiABEiABEuAfOwxMg4dzSqsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped and Reconstructed Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAD0UlEQVR4nO3bzYscRRgG8F9POhN1NyB+JRiNXhTjByqKF0W85KL4EfGW26Kg4sWTHhQv4sV/QFG8eBBy8OBJRcSLIEJAvPiJiIJRSHRjzKJJdieHqu6Z3u6eng1C7aFemKG3urb6mace3vett6qLMWzET9MKmECJZThZ3Zu0ei9s02FhdOED/T+WASQHUOyG03M6sAXJ9XQs+v8jOQMZQLGDuXM8ostJtW1IJz06SM5ABpAcQFlro0dEbf1N6JLU0EA9lpyBDKCcsGnaOua4mO0zJ7J03B9SRHIGMoDkAMqdcKb6czJ7MVXTkJKuwSr8iHU4gucZzqiSM5ABFCUzEacxWUMep4S34SbcWTVvwAncA79XTT1iSM5ABpAcQNmWxZD2wBV4GR6DJUF2Pom33I2H4Vv4XJ/Q0zOQASQHUAwh2EEMcJVdDO/g8frWB3gN/sRBeB274Ws4ZMYlNhP95AxkAOVQxtKYflfiXngUx+EA3Ic/CMWmJwlCmcDTcKw1ENHjJWcgA0gOoCMa9tgSPIcX4Hv8AnsJQbCAy/EGvCl4JXvYJMD6N2cRbg8Ag8GotiW4DkfhMzxDnMjjuIPgdh6ES0R5/DUwbHIGMoDkAOaJcOqj6kz9fnxISGt+Iu7mncPVMDZTX/8ZbhwAkJyBDKDcWvejeAleEWd9GdbEOtOoGnIdby0yYnIGMoDkABaLho2yZQG7RLW9C4dxM3wshs1VPEVYtc2z5AxkAMkBdIiwXi1O2s1tGxNWgscIutyAT7FCLAvMseQMZAAdW7cd+3O1IhpCKIi5939iNNwpauArsX4wZMkZyACSAyjHBG9yihk8pbiqr5f27d3kA8Qi1IoQH/2DuwiFgnZZamojuT6wXQCU55jZuiU4Ew8I6U2wdiA6hFfh2qrlV3gPN8BH/Y8sCOFrnW3AQAaQHMDiRSoEvCvwLG4hSmpV3FrZi7MLPTcOlp6BDGCx+kCJfYT65PtwvQj+JKF+WbDY/Nf1g+yIMgDMOqKe03JXEQLcbXArniAUAc4S0/JaUoOPk88PZACbbeoJm9rbhxfhIYISxwSlfQHfCGcINuXuPbYHfxNyv2bn5AxkAB3RcAyPiKnPrqr5NOFY0mHCQu7Uwk/5V6ghdPze5AxkAMkBlCPCTu8aIVIdIYS9UdXiO9H/7I+d/dY9XKOsXr/I50x10X5bITkDGUDIiJaF49l+wO2EM9oXEfJvX17Y2I2rDULtoZm6J2cgA0gOYLA+sLWX6C4Tw97aogMlZyAD6KsPNN8x6bai/prgUsL5teYZzfyOSQYwZOcBp/S1MGqg4L0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "6540 images left\n",
            "Iteration: 577 -- Reached Defined Optimum -- Final Loss: 49.986900329589844\n",
            "Original Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAACBElEQVR4nO3ar4sUYRgH8M/6A/Q4DIfIYRKDSWQxeRY1aBCjJkGwGEwaRU02iyD4B5xJg2AwyOGPQ0QERQzCYRWDwWSQRQyGd0Y4lJl3hz0eDp4nLMPsy+6HLw8z7z6zo5HY2hL8/QlIQAISkIAEJCABCUhAAhIQD9hWvXIB1rDcnnkNH+AAXgwDhCeQgHBAXROewn7YiftwD0swhnc4ATenBoQnkIBwQG8T7oGDuAEv8QmOrVt0GvuGAcITSMCob1C5F75gB/zuWb2Gz3Abb2oA4QkkIBzQdSHaDQ/gGQ7RbMD+X0dgq7I/M6kFhCeQgK4eeARHYQ6/ej7pMGXb9JTOZllf4QkkIBzQ1YQn4Sfld+G3nk9aGAYITyABXT1wgeb6s4JL8PafRXNwRdkZD6jwBBIQDujdlo/hPVbhCW7BRw3+DmVkMIFdmrvhmVpAeAIJCAf0zge+U3rrBxxvTy9p8NspQ6qHcHdqQHgCCejtga/twVm4iOcwr5mWv2rfv9wejCmjzZUaQHgCCQgH1D+yefz3pacW4Zxsws0CqO+B6ep67cLwBBKQgAQkIAEJ2Ki74VVcq1kYnkACwgEb0IQTykPkqgpPIAGz64H59mCV5t9ONRWeQALCAb3T8upapIy0clq+yQCz64FlOG/aa1t4AgkIB8yuCQdWeAIJSEACwgF/AMgJOatT1ttvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped and Reconstructed Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAADm0lEQVR4nO3bMYhcRRjA8d/bfblTE+ViQFAxEFECIopiYbAwrSKiForaCIIWWtnY2ATsLQIq1oqFEq1sRANBGxER7RS1iAYLUdScerk712Lmze27t/v2XRRmhfmKx87MMvPf733M9833zVYju2UNfsPfnSFQNR8mUMN204JRM1al1tbMiUbpkVMKQHaAutu1zZRJdaU91LWvZLtVdzzZZhrProECUO2dINlA1W0l2Ycx/DV/ogMsgQYKQHaA4UbYszWFmVofJuKPm+NSL8dRlkADBaDue1F7l+tF9/OTuXvQQbgL77MEGigA2QEuwhsmmbDbCQ6RMVNmn10DBWC4DVwC7+F+uAMn4Qf4oGn1xD9zJLsGCkB2gIVGOII3cSscabp/xEPwBNyG2+E4PmUP/jW7BgpAdoAZRjginO0Owym4VgikrYv+7wb8AqvwJB6FDdwDfw4FyK6BArBjAyk+XyPkjl6EBwixz0n4TIymtzszPQyv4hxh2+p+Z4Zk10AByA6wywjXxP1lBbfAR3AGzxOMcI6swFd4G07g90Vrp2VzSgHYSVavws9NawvPEg9SL+PznlkqPAiHRK/0woC1xyyBBgpAdoC6IsQ/m63+CS40H7yCd3tmOYR7CQfIdhK9Ry5jCTRQAOr0vNAZ+wQeg7O4Dz4Wqro7xbAr4FI80vR82/RsLFh7nSXQQAHIDlAll9QttO6Dd+AYviakBO6EL2Ovx+FuU1vPcULo1N7aZqzdLJ1VCkB2gHrCvFPcJjHAOoMv4HQzdlAMxMfwjFCk8Zy4wa1abIQTlkADBWBYtrwSs+U34Rp4GlcSneA5cdt6Q/SPT+G7IVNn10AByA7wb8p2bakJ4dp5gm2e7vt6I9k1UABmXGa7SLkO/hC9Um3gGS27BgpAdoB5RliZjliGFOF+JVSS9xMynjPsb9SZLLsGCkBdNxibTN16Te9v3Dy2Wt2pke6QHCUESfvhZrzVXS793nKHpAAkqa8mHOlPEY5VM65u99iepnUjIS+1AR/OXq6Vfyj5gQIA6u/h9flfmMxudT3dWcIuuE4sIC+Qkh9YDoB6SGljkBwhBFhXEa7cvcTiYCq7BgpAdoB5+YFV/SWX1j+WdmrHr4lnw8Pin1iKES49wIyjWbrM1mcD7YhohG8I93uPEW+cDJHsGigA2QH+u0QlptIC55XbdP8XgOoAIb84WEaE8HeDqRfd98bTtrUiJiJKfqAAJPkHHYWiZ4sz7tEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "6539 images left\n",
            "Iteration: 435 -- Reached Defined Optimum -- Final Loss: 49.99810028076172\n",
            "Original Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAACHElEQVR4nO3Zu2sUURTH8c/GKIKCgpViYSGirSiIYGerNloYLHwhChEECxEbsVUs1NrCQixC/gDBVNqkEmxDwM5ysQgovop57G4a584Ix+L8imXuPM5+97eHc+89MxqJ1Vzw9ydAAiRAAiRAAiRAAiRAAiRAAsQDzA949iTshw0swxg7YC++dgkS7kAChAOMevYHtmEVDlP9jF8z1y9hqUugcAcSIBygsBJux2V4qoYfwyKuwd3mxvWuEcMdSIDCQvQYt5vBEqzAy/4A4Q4kQDhAYSE60xyM8ZnN+beIW3AOa10ihjuQAN1zYAEOqpc+7/GgvXYUL+BYc+aQzIEE6KjC2XBOtRG0FVva089wY+bGq3jdNWKoEqBwMvrSHOzCTrgPN01tzZ5QL5S6KNyBBAgHKEzCNdXUZ11VfiabsVYbTGXr3xTuQAKEAxTOhudxh6m1FzY1qT7B8a4Rwx1IgL6Nyg/Y3Y6O4AdV/bkOb7sGCncgAcIB+ibhrL6rC9EJfCx6NNyBBBjy2g5VV2iid9hT9Hy4AwkQDjAkCQ9QvUOZ6FRxkHAHEmBIDlyBe81oH/W2rEjhDiRAOMCQFdHP9mMeD+FRcZBwBxIgHGBIJbxA3RJfwat+QcIdSIC+heg5LlK3zE+rXuT1ULgDCRAO0DcJf+NbOzqroC01q3AHEmBwf+ANvf9//gMHEiAc4N80Kgco3IEESIAECAf4AznVPdPzWH32AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped and Reconstructed Image\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAADS0lEQVR4nO3bPagdRRTA8d/ed/OiERURCUqKFMGvQiMBG0ERC0sLizTahiAIFjYBrbWzDtgLdhYJJIWm0dZGBPErgRAQxRjh5QWf3msxZza57t69e1/AucWcYtk7Mzv7f2cP55w5M6+ZCGlgG7PcsmcdaSeat5euNJ2uSf/A/08qQHGApjRB6fdXANNRo+bCUY2R9m+aDY3qjC4lFaA4wH8cUWtpS6LZqCnzzRb8s2K24hqoAM1iRsRdff59SHENVIDiALej4UAi7QD+ZmSAW0uKa6ACDGVEDWEae/pQn4djcNYdS7t7iRjUP+VczNbkm6JSAYoDpGi4ViQ8CN/npw7n5j3Yxafw1uJb8tRbwj4neDDfFJUKUBxgug1/rfPIY/Bo/nUN3sR5uEcY4WI1ai587iy6zPAHG6CBCjBUI1r0Sg1cxEskj/I4PAkX8AC8jLfF5ZtV72YDNFABigOMK1I1YrV/PN+8I3zQT3nMdZLdHoMzeCO3LJE2NysqFWDakL7xqlXXYVI29Dt8jlttXyPc0gd4GH4dC1BcAxWgOEByRGOy8ROkjOc8/LDQdx9Ok5zQLnzUP2s3/y+ugQow7aY9PYkQvCb6zsAh4ZoukXzTc3ngl3Cl86YJPIXvSG5v1jaXlApQHKAnLb9dm1qQF+ATyf58jJNwpH0EfhGlgc86z2+Tqp3t0qxmRBUAY9eGjYibD+Hn3LdD1EVviLLBRbxOhMSh96pGuCkA45ZmxI7JDp6FZ/AFUV76LQ96xMhtv5oRVYBWxm3ZtHITP8oXeJ+0rTeD99y5ZBwjxTVQAe7mMNsW/EkqG3wFr4itkyVHn7rNxTVQAYoDjI+GXTlAmNxBUTHYMxwNu3ZZXAMVYMgRbRExpv+zvgrnFkaPPnbZSnENVIDiAD0nKluLmxLZeM9zYsW/Q/I/Z0kbKetJcQ1UgOIA+03JXsSHxHGqy3iaNQ8iYAM0UAH2mxFdFYUC8K7wWFNLXVdX6vmBCoB9GeEETom0fAZHhUmNNkBqoXJTAMbbQHvQYArf5uY5aaNkyUHeVVJcAxWgOMDKjGjJJl4jNkuegK/zmCVrw+6WdCvFNVABhmzgfiLY7Frj30t6VnQDiVJxDVSA4gD/AqAnjHFptD9/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "6538 images left\n",
            "Loss at Iteration 4000 too high -- Loss: 87.75917053222656\n",
            "Could not find optimal region within the defined iteration count. Retry (1) with another random z...\n",
            "6538 images left\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda import reset_max_memory_allocated\n",
        "def create_cp(iteration_number):\n",
        "  print(\"CREATING CHECKPOINT...\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  shutil.make_archive(f\"/content/drive/MyDrive/Colab/data/latent_space_mappings_cp/latent_space_mappings_cp{iteration_number}\", 'zip', \"/content/data/latent_space_mappings\")\n",
        "\n",
        "def save_to_drive(mapped_z, iteration_number, csv_path):\n",
        "  torch.save(mapped_z, f'/content/drive/MyDrive/Colab/data/latent_space_mappings/mapped_z_{iteration_number}.pt')\n",
        "  shutil.copy(csv_path, \"/content/drive/MyDrive/Colab/data/latent_space_mappings/latent_space_mappings.csv\")\n",
        "\n",
        "base_folder = \"/content/data/latent_space_mappings\"\n",
        "csv_path = os.path.join(base_folder, \"latent_space_mappings.csv\")\n",
        "\n",
        "\n",
        "prepare_target_folder = True\n",
        "if prepare_target_folder:\n",
        "    if not os.path.exists(base_folder):\n",
        "        os.makedirs(base_folder, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(base_folder):\n",
        "        shutil.rmtree(base_folder)\n",
        "        os.makedirs(base_folder, exist_ok=True)\n",
        "\n",
        "    with open(csv_path, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        fields = [\"filename\", \"label\", \"reconstruction_loss\"]\n",
        "        writer.writerow(fields)\n",
        "\n",
        "# Start mapping\n",
        "map_anomalies = True\n",
        "map_normals = True\n",
        "t = transforms.ToPILImage()\n",
        "lsm: LatentSpaceMapper = LatentSpaceMapper(generator=generator, discriminator=discriminator, device=device)\n",
        "mapped_images = []\n",
        "cp_counter = 0\n",
        "counter = len(data)\n",
        "\n",
        "i = 0\n",
        "retry_counter = 0\n",
        "\n",
        "for d in iter(data):\n",
        "    if i % 50 == 0 and i != 0:\n",
        "        clear_output()\n",
        "        print(\"Cleared Output...\")\n",
        "\n",
        "    print(f\"{counter} images left\")\n",
        "\n",
        "    max_retries = 20\n",
        "    opt_threshold=50\n",
        "    ignore_rules_below_threshold=55\n",
        "    immediate_retry_threshold=65\n",
        "    max_opt_iterations=25000\n",
        "\n",
        "    if (d[1] == 6 and map_anomalies) or (d[1] == 9 and map_normals):\n",
        "        mapped_z, reconstruction_loss, retry = lsm.map_image_to_point_in_latent_space(d[0][0],\n",
        "                                                                            batch_size=1,\n",
        "                                                                            max_opt_iterations=max_opt_iterations,\n",
        "                                                                            plateu_threshold=0.0005,\n",
        "                                                                            check_every_n_iter=4000,\n",
        "                                                                            learning_rate=0.1,\n",
        "                                                                            print_every_n_iters=5000,\n",
        "                                                                            retry_after_n_iters=10000,\n",
        "                                                                            ignore_rules_below_threshold=ignore_rules_below_threshold,\n",
        "                                                                            opt_threshold=opt_threshold,\n",
        "                                                                            immediate_retry_threshold=immediate_retry_threshold)\n",
        "\n",
        "        if retry:\n",
        "            if retry_counter == max_retries:\n",
        "                retry_counter = 0\n",
        "                i+=1\n",
        "                counter-=1\n",
        "                print(\"Retry Limit reached. Moving on to next sample\")\n",
        "                print('Original Image That Could Not Be Mapped')\n",
        "                t(d[0][0]).resize((128, 128), PIL.Image.NEAREST).show()\n",
        "                print('-----------------------')\n",
        "                continue\n",
        "            else:\n",
        "                retry_counter += 1\n",
        "                print(f\"Could not find optimal region within the defined iteration count. Retry ({retry_counter}) with another random z...\")\n",
        "                continue\n",
        "\n",
        "        retry_counter = 0\n",
        "        mapped_images.append(mapped_z)\n",
        "        with open(csv_path, 'a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            fields = [f'mapped_z_{counter}.pt', d[1], math.floor(reconstruction_loss)]\n",
        "            writer.writerow(fields)\n",
        "\n",
        "        torch.save(mapped_z, os.path.join(base_folder, f'mapped_z_{counter}.pt'))\n",
        "        save_to_drive(mapped_z, counter, csv_path)\n",
        "        cp_counter += 1\n",
        "        if cp_counter % 50 == 0:\n",
        "            create_cp(counter)\n",
        "            clear_output\n",
        "\n",
        "        print('Original Image')\n",
        "        display(t(d[0][0]).resize((128, 128), PIL.Image.NEAREST))\n",
        "        print('Mapped and Reconstructed Image')\n",
        "        original_img = generator(mapped_z).cpu()\n",
        "        display(t(original_img[0]).resize((128, 128), PIL.Image.NEAREST))\n",
        "        print('-----------------------')\n",
        "\n",
        "    i+=1\n",
        "    counter-=1\n",
        "\n",
        "create_cp(0)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-19T13:17:18.568254955Z",
          "start_time": "2023-06-19T13:17:09.250792045Z"
        },
        "id": "Y611jafizmex",
        "outputId": "3cbc555d-3cf2-46ea-e9a6-26fd682ff1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHpnpEgVeeV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}