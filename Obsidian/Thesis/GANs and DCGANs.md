[GANs](https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f)
[DCGAN](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
[[Generative Adversarial Nets]] (First paper on GANs - Goodfellow et. al.)
[[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks]] (First paper on DCGANs - Radford et. al.)

### DCGAN

- Consists of two Convolutional Neural Networks
	- Discriminator $D(x)$
		- Binary classifier
		- Optimized to recognize images that are not generated by the generator
		- Result should be high when image comes from real/training dataset and low when it comes from the generator
	- Generator $G(x)$
		- Optimized to generate image data that, for the discriminator, is indistinguishable from the real data
	- $x$ is image data
	- $z$ is latent space vector, sampled from standard normal distribution
	- $G(z)$ is therefore the generator that maps the latent vector $z$ to data-space
		- Its goal is to estimate the distribution of the training data $p_{data}$ so it can generate fake samples from that estimated (learned) distribution $p_{g}$ 
	- $D(G(x))$ is the scalar probability that $G(x)$ is real
		- $D$ and $G$ play a minmax game 
			- $D$ tries to maximize the probability that it correctly identifies fake images
				- $logD(x)$
			- $G$ tries to minimize the same
				- $log(1 - D(G(x)))$
- Overall loss function:

$\Large\underset{G}{min}\underset{D}{max}V(D,G)=\mathbb{E}_{x∼pdata}​(x)​[logD(x)]+\mathbb{E}_{z∼p_{z}​(z)​}[log(1−D(G(z)))]$

